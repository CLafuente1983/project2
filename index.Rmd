---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Cristina Lafuente, cel726

### Introduction 

I chose the Glass Identification dataset from the MLBench repository for this project. I find this data particularly interesting because it breaks down the chemical composition of the glass into its base components and then, through data analysis, a model can be created which can be used to predict what a specific piece of glass came from. In this data set, there is a column listed as "RI" which is the Refractive index, commonly called 'n' and used in Snell's law to determine by how much light is bent through the glass. In addition to the Refractive Index, the dataset also has the amount of each of the elements Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10), Mg: Magnesium, Al: Aluminum, Si: Silicon, K: Potassium, Ca: Calcium, Ba: Barium, Fe: Iron. Finally, there is a category called "Type" where type is listed as a number 1-7. Those correspond with: 
    1 building_windows_float_processed
--  2 building_windows_non_float_processed
--  3 vehicle_windows_float_processed
--  4 vehicle_windows_non_float_processed (none in this database)
--  5 containers
--  6 tableware
--  7 headlamps
In addition to these variables, I also added a binary variable to express simply whether this piece is window glass or not. As can be seen from the categories, window glass would correspond with anything of category 4 or lower while non-window glass would be category 5 or higher. 
  I found this dataset and it seemed particularly interesting because glass can be formed by nature (for example by lightning in the sand) as well as made by people and manipulated to form functional items like windows or simply to make art. It is interesting to think that it can be evaluated and sorted into different categories or types of data based on its chemical composition and refractive index.
  ![lightning](./img/sand_glass.jpg)
  This is glass formed by lightning that has struck sand.
   ![Daniela Forte Glass Sculpture](./img/daniela_forti_sculpture.jpg)
   This is a Daniela Forti Glass Sculpture. Both are glass but would have significantly different refractive indexes, and chemical compositions and should be able to be categorized differently by predictive models. 

```{R}
## some libraries to make things easier and make sure that data can be read and used 
library(tidyverse)
library(tidyr)
library(dplyr)
## the library that contains the data set
library(mlbench)
data(Glass) ## this is the data set to be used here
str(Glass)

glass<-(Glass) ## save the dataset as simply glass in order to use it more easily
dim(Glass) ## examine the dimensions 
head(glass) ## get a preview of the dataset
levels(glass$Type) ## see what levels are contained within the dataset
#head(glass)

## add a binary column that expresses only whether the glass is window glass or not
glass<-glass %>% 
  mutate(Window = case_when(as.numeric(Type) <= 4 ~ 1, as.numeric(Type) > 4 ~ 0))

##  change glass$Type to numeric since they were factor
glass$Type <- as.numeric(glass$Type)

#glass$Type

## take a look at the current dataset
glimpse(glass)
```

### Cluster Analysis

```{R}
library(cluster)

## set up the wss graph with kmeans.
## empty vector wss
wss<-vector() 
## function that goes from one to ten to test for possible k from 1 to 10
for(i in 1:10){
  ## remove the Type and Window from the dataframe since they are categorical variables then pass it to kmeans
temp<- glass %>% select(-(Type)) %>% select(-Window) %>% kmeans(i)
wss[i]<-temp$tot.withinss
}
## plot the whole thing
ggplot()+geom_point(aes(x=1:10,y=wss))+geom_path(aes(x=1:10,y=wss))+
  xlab("clusters")+scale_x_continuous(breaks=1:10)
```
Using the older method of the WSS graph for determining k, it is not completely clear how many clusters there should be. The "elbow" looks like it could be at 4 though it is also possible that it could be 7 since there is a continued sharp decline and then a rapid change in direction. Given that, I will also, use the sillhouette method.

```{r}
clust_dat <- glass %>% select(-(Type)) %>% select(-Window)
sil_width<-vector() #empty vector to hold mean sil width
for(i in 2:10){  
  kms <- kmeans(clust_dat,centers=i) #compute k-means solution
  sil <- silhouette(kms$cluster,dist(clust_dat)) #get sil widths
  sil_width[i]<-mean(sil[,3]) #take averages (higher is better)
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
```
Here it looks like 2 might be the best it can possibly be so I will attempt the clustering with k=2.

```{r}
set.seed(322)
kmeans1 <- clust_dat %>% kmeans(2)
kmeans1

#you can grab lots of stuff
kmeans1$size
kmeans1$centers
kmeans1$cluster
kmeans1$betweenss
kmeans1$tot.withinss
```

```{r}
pam1<-clust_dat%>%pam(k=2)
pam1
```
```{r}
pamclust<-clust_dat%>%mutate(cluster=as.factor(pam1$clustering))
pamclust%>%group_by(cluster)%>%summarize_if(is.numeric,mean,na.rm=T)
```

```{r}
glass%>%slice(pam1$id.med)
```



Here, K means and PAM clustering produce very nearly the same results, we can see that the medoids of the 2 clusters are type 3 and type 6 glass. Those types correspond with vehicle windows and tableware. That also makes one of the clusters centered on windows and one non-windows. A few of the elements are very close together while others are farther apart. RI is different by only 0.00126 while Mg is different by 3.57. 

```{r}
pam1$silinfo$avg.width
plot(pam1,which=2)
```
Average silhouette width is 0.56 which is reasonable structure based on the standard cutoffs. This is also what I would have expected to get based on the previous graph which showed approximately 0.56 average silhouette width k=2. Based on that same graph, for k=4 or k=7 I could have expected to achieve silhouette width of only .35-0.41 which would have been considered weak, possibly artificial in origin. 

```{R fig.width=12, fig.height=6}
library(GGally)
final <- clust_dat %>% mutate(cluster=as.factor(pam1$clustering))
ggpairs(final, aes(color=cluster))
#ggpairs(final, columns =1:4, aes(color=cluster))
```
Interestingly, from this correlation matrix, we can see that there really isn't strong correlation between most of the elements and each other. However, there is actually strong correlation between Ca and RI. Additionally, in many cases, the clusters aren't particularly clear. Mg, identified above as being most different, has clusters significantly separated from one another. K has some decent separation as well.

    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
# linear classifier code here
```

```{R}
# cross-validation of linear classifier here
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
```

```{R}
# cross-validation of np classifier here
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
```

```{R}
# cross-validation of regression model here
```

Discussion

### Python 

```{R}
library(reticulate)
```

```{python}
# python code here
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




